24-10-07 11:44:47.915 : ===================== Selected training parameters =====================
24-10-07 11:44:47.915 : Namespace(opt='/home/asif/Documents/kernel_est/experiment5/pl_idenet/setting1/train/train_setting1_x4.yml', color=True, wiener_kernel_size=(5, 5), wiener_output_features=24, numWienerFilters=4, wienerWeightSharing=True, wienerChannelSharing=True, alphaChannelSharing=True, alpha_update=True, lb=1e-05, ub=0.01, wiener_pad=True, wiener_padType='symmetric', edgeTaper=True, wiener_scale=False, wiener_normalizedWeights=True, wiener_zeroMeanWeights=True, kernel_size=(5, 5), conv_init='dct', pad='same', padType='symmetric', in_nc=3, nf=64, resdnet_depth=5, patch=1, embed_dim=96, depths=[4, 4, 4, 4], num_heads=[4, 4, 4, 4], window_size=8, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.1, ape=False, patch_norm=True, use_checkpoint=False, upscale=4, img_range=1.0, upsampler='', resi_connection='1conv', train_stdn=[0.0], test_stdn=[0.0], upscale_factor=4, trainBatchSize=2, testBatchSize=1, niter=500000, use_bn=False, cuda=True, gpu_ids=[0], seed=123, use_filters=False, resume=True, resume_start_epoch=0, pretrainedModelPath='pretrained_nets/idenet/2853_final_G.pth', pretrain=False, tile=128, tile_overlap=32, whole=True, imdbTrainPath='datasets/', imdbTestPath='datasets/', patch_size=32, rgb_range=1, is_train=True, is_mixup=True, use_chop=False, alpha=1.2, numWorkers=4, lr_G=0.0001, beta1_G=0.9, beta2_G=0.999, eps_G=1e-08, weightdecay_G=0, lr_D=0.0001, beta1_D=0.9, beta2_D=0.999, eps_D=1e-08, weightdecay_D=0, amsgrad=False, lr_milestones=[250000, 400000, 450000, 475000], lr_gamma=0.5, lr_restart=None, lr_restart_weights=None, warmup_iter=-1, D_update_ratio=1, D_init_iters=0, pixel_criterion='l1', pixel_criterion_char='charbonnair', pixel_criterion_kernel='l1', pixel_criterion_GT_kernel='l1', feature_criterion='l1', tv_criterion='l1', gan_type='gan', pixel_weight=1.0, pixel_kernel_weight=1.0, pixel_weight_char=1.0, pixel_kernel_gt_weight=1.0, feature_weight=1.0, layer_weights={'conv1_2': 0.1, 'conv2_2': 0.1, 'conv3_4': 1, 'conv4_4': 1, 'conv5_4': 1}, tv_weight=1.0, gan_weight=0.1, saveTrainedModelsPath='trained_nets', save_path_training_states='/training_states/', save_path_netG='/netG/', save_path_netD='/netD/', save_path_best_psnr='/best_psnr/', save_path_best_lpips='/best_lpips/', saveImgsPath='results', saveLogsPath='logs', save_checkpoint_freq=50, saveBest=True).
24-10-07 11:44:47.915 : ===================== Loading dataset =====================
24-10-07 11:48:05.457 : ===================== Selected training parameters =====================
24-10-07 11:48:05.457 : Namespace(opt='/home/asif/Documents/kernel_est/experiment5/pl_idenet/setting1/train/train_setting1_x4.yml', color=True, wiener_kernel_size=(5, 5), wiener_output_features=24, numWienerFilters=4, wienerWeightSharing=True, wienerChannelSharing=True, alphaChannelSharing=True, alpha_update=True, lb=1e-05, ub=0.01, wiener_pad=True, wiener_padType='symmetric', edgeTaper=True, wiener_scale=False, wiener_normalizedWeights=True, wiener_zeroMeanWeights=True, kernel_size=(5, 5), conv_init='dct', pad='same', padType='symmetric', in_nc=3, nf=64, resdnet_depth=5, patch=1, embed_dim=96, depths=[4, 4, 4, 4], num_heads=[4, 4, 4, 4], window_size=8, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.1, ape=False, patch_norm=True, use_checkpoint=False, upscale=4, img_range=1.0, upsampler='', resi_connection='1conv', train_stdn=[0.0], test_stdn=[0.0], upscale_factor=4, trainBatchSize=2, testBatchSize=1, niter=500000, use_bn=False, cuda=True, gpu_ids=[0], seed=123, use_filters=False, resume=True, resume_start_epoch=0, pretrainedModelPath='pretrained_nets/idenet/2853_final_G.pth', pretrain=False, tile=128, tile_overlap=32, whole=True, imdbTrainPath='datasets/', imdbTestPath='datasets/', patch_size=32, rgb_range=1, is_train=True, is_mixup=True, use_chop=False, alpha=1.2, numWorkers=4, lr_G=0.0001, beta1_G=0.9, beta2_G=0.999, eps_G=1e-08, weightdecay_G=0, lr_D=0.0001, beta1_D=0.9, beta2_D=0.999, eps_D=1e-08, weightdecay_D=0, amsgrad=False, lr_milestones=[250000, 400000, 450000, 475000], lr_gamma=0.5, lr_restart=None, lr_restart_weights=None, warmup_iter=-1, D_update_ratio=1, D_init_iters=0, pixel_criterion='l1', pixel_criterion_char='charbonnair', pixel_criterion_kernel='l1', pixel_criterion_GT_kernel='l1', feature_criterion='l1', tv_criterion='l1', gan_type='gan', pixel_weight=1.0, pixel_kernel_weight=1.0, pixel_weight_char=1.0, pixel_kernel_gt_weight=1.0, feature_weight=1.0, layer_weights={'conv1_2': 0.1, 'conv2_2': 0.1, 'conv3_4': 1, 'conv4_4': 1, 'conv5_4': 1}, tv_weight=1.0, gan_weight=0.1, saveTrainedModelsPath='trained_nets', save_path_training_states='/training_states/', save_path_netG='/netG/', save_path_netD='/netD/', save_path_best_psnr='/best_psnr/', save_path_best_lpips='/best_lpips/', saveImgsPath='results', saveLogsPath='logs', save_checkpoint_freq=50, saveBest=True).
24-10-07 11:48:05.457 : ===================== Loading dataset =====================
24-10-07 11:48:05.458 : training dataset:     1
24-10-07 11:48:05.458 : training loaders:     0
24-10-07 11:50:33.681 : ===================== Selected training parameters =====================
24-10-07 11:50:33.682 : Namespace(opt='/home/asif/Documents/kernel_est/experiment5/pl_idenet/setting1/train/train_setting1_x4.yml', color=True, wiener_kernel_size=(5, 5), wiener_output_features=24, numWienerFilters=4, wienerWeightSharing=True, wienerChannelSharing=True, alphaChannelSharing=True, alpha_update=True, lb=1e-05, ub=0.01, wiener_pad=True, wiener_padType='symmetric', edgeTaper=True, wiener_scale=False, wiener_normalizedWeights=True, wiener_zeroMeanWeights=True, kernel_size=(5, 5), conv_init='dct', pad='same', padType='symmetric', in_nc=3, nf=64, resdnet_depth=5, patch=1, embed_dim=96, depths=[4, 4, 4, 4], num_heads=[4, 4, 4, 4], window_size=8, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.1, ape=False, patch_norm=True, use_checkpoint=False, upscale=4, img_range=1.0, upsampler='', resi_connection='1conv', train_stdn=[0.0], test_stdn=[0.0], upscale_factor=4, trainBatchSize=2, testBatchSize=1, niter=500000, use_bn=False, cuda=True, gpu_ids=[0], seed=123, use_filters=False, resume=True, resume_start_epoch=0, pretrainedModelPath='pretrained_nets/idenet/2853_final_G.pth', pretrain=False, tile=128, tile_overlap=32, whole=True, imdbTrainPath='datasets/', imdbTestPath='datasets/', patch_size=32, rgb_range=1, is_train=True, is_mixup=True, use_chop=False, alpha=1.2, numWorkers=4, lr_G=0.0001, beta1_G=0.9, beta2_G=0.999, eps_G=1e-08, weightdecay_G=0, lr_D=0.0001, beta1_D=0.9, beta2_D=0.999, eps_D=1e-08, weightdecay_D=0, amsgrad=False, lr_milestones=[250000, 400000, 450000, 475000], lr_gamma=0.5, lr_restart=None, lr_restart_weights=None, warmup_iter=-1, D_update_ratio=1, D_init_iters=0, pixel_criterion='l1', pixel_criterion_char='charbonnair', pixel_criterion_kernel='l1', pixel_criterion_GT_kernel='l1', feature_criterion='l1', tv_criterion='l1', gan_type='gan', pixel_weight=1.0, pixel_kernel_weight=1.0, pixel_weight_char=1.0, pixel_kernel_gt_weight=1.0, feature_weight=1.0, layer_weights={'conv1_2': 0.1, 'conv2_2': 0.1, 'conv3_4': 1, 'conv4_4': 1, 'conv5_4': 1}, tv_weight=1.0, gan_weight=0.1, saveTrainedModelsPath='trained_nets', save_path_training_states='/training_states/', save_path_netG='/netG/', save_path_netD='/netD/', save_path_best_psnr='/best_psnr/', save_path_best_lpips='/best_lpips/', saveImgsPath='results', saveLogsPath='logs', save_checkpoint_freq=50, saveBest=True).
24-10-07 11:50:33.682 : ===================== Loading dataset =====================
24-10-07 11:50:33.682 : training dataset:     1
24-10-07 11:50:33.682 : training loaders:     0
24-10-07 11:50:33.682 : testing dataset:     1
24-10-07 11:50:33.682 : testing loaders:     1
24-10-07 11:50:33.682 : ===================== Building model =====================
24-10-07 11:50:37.161 : Network G structure: DataParallel - DEblurSRResDNet, with parameters: 4,307,982
24-10-07 11:50:37.161 : DEblurSRResDNet(
  (netDe): SRResDNet(
    (netT): SwinIR(
      (conv_first): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=96, input_resolution=(32, 32), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.007)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.013)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.020)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(224, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
          (pgm): PromptGenBlock(
            (linear_layer): Linear(in_features=96, out_features=128, bias=True)
            (conv3x3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (pim): PromptInteractionBlock(
            (transformerblock): TransformerBlock(
              (norm1): LayerNorm(
                (body): WithBias_LayerNorm()
              )
              (attn): Attention(
                (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              )
              (norm2): LayerNorm(
                (body): WithBias_LayerNorm()
              )
              (ffn): FeedForward(
                (project_in): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
                (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              )
            )
          )
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=96, input_resolution=(32, 32), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.027)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.033)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.040)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.047)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
          (pgm): PromptGenBlock(
            (linear_layer): Linear(in_features=96, out_features=128, bias=True)
            (conv3x3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (pim): PromptInteractionBlock(
            (transformerblock): TransformerBlock(
              (norm1): LayerNorm(
                (body): WithBias_LayerNorm()
              )
              (attn): Attention(
                (qkv): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (qkv_dwconv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
                (project_out): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              )
              (norm2): LayerNorm(
                (body): WithBias_LayerNorm()
              )
              (ffn): FeedForward(
                (project_in): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (dwconv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
                (project_out): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              )
            )
          )
        )
        (2): RSTB(
          (residual_group): BasicLayer(
            dim=96, input_resolution=(32, 32), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.053)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.060)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.067)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.073)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
          (pgm): PromptGenBlock(
            (linear_layer): Linear(in_features=96, out_features=128, bias=True)
            (conv3x3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (pim): PromptInteractionBlock(
            (transformerblock): TransformerBlock(
              (norm1): LayerNorm(
                (body): WithBias_LayerNorm()
              )
              (attn): Attention(
                (qkv): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (qkv_dwconv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
                (project_out): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              )
              (norm2): LayerNorm(
                (body): WithBias_LayerNorm()
              )
              (ffn): FeedForward(
                (project_in): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (dwconv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
                (project_out): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              )
            )
          )
        )
        (3): RSTB(
          (residual_group): BasicLayer(
            dim=96, input_resolution=(32, 32), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.080)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.087)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.093)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.100)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
          (pgm): PromptGenBlock(
            (linear_layer): Linear(in_features=96, out_features=128, bias=True)
            (conv3x3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (pim): PromptInteractionBlock(
            (transformerblock): TransformerBlock(
              (norm1): LayerNorm(
                (body): WithBias_LayerNorm()
              )
              (attn): Attention(
                (qkv): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (qkv_dwconv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
                (project_out): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              )
              (norm2): LayerNorm(
                (body): WithBias_LayerNorm()
              )
              (ffn): FeedForward(
                (project_in): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (dwconv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
                (project_out): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              )
            )
          )
        )
      )
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_last): Conv2d(96, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (deblur): WienerDeblurNet(, wiener_kernel_size = (5, 5), wiener_output_features = 24, wienerWeightSharing = True, edgeTaper = True
    (noise_estimator): Wmad_estimator()
    (k_embd): M_kemb(
      (linear): Linear(in_features=441, out_features=128, bias=True)
      (act): ReLU()
    )
    (conv_before_upsample): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (upsample): Upsample(
      (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): PixelShuffle(upscale_factor=2)
      (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): PixelShuffle(upscale_factor=2)
    )
    (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (netK): KernelEstimator(
    (conv_1): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
    )
    (res_block_1): Sequential(
      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (max_pool_1): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (conv_2): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
    )
    (res_block_2): Sequential(
      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (maxpool_2): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (conv_3): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
    )
    (transpose_1): Sequential(
      (0): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
    )
    (conv_4): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
    )
    (res_block_3): Sequential(
      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (transpose_2): Sequential(
      (0): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
    )
    (conv_5): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
    )
    (res_block_4): Sequential(
      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (conv_6): Sequential(
      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))
      (1): AdaptiveAvgPool2d(output_size=1)
      (2): Conv2d(128, 441, kernel_size=(1, 1), stride=(1, 1))
      (3): Softmax(dim=None)
    )
  )
)
24-10-07 11:50:37.161 : Network K structure: KernelEstimator, with parameters: 350,137
24-10-07 11:50:37.161 : KernelEstimator(
  (conv_1): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
  )
  (res_block_1): Sequential(
    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (max_pool_1): Sequential(
    (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv_2): Sequential(
    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
  )
  (res_block_2): Sequential(
    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (maxpool_2): Sequential(
    (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv_3): Sequential(
    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
  )
  (transpose_1): Sequential(
    (0): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
  )
  (conv_4): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
  )
  (res_block_3): Sequential(
    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (transpose_2): Sequential(
    (0): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
  )
  (conv_5): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
  )
  (res_block_4): Sequential(
    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (conv_6): Sequential(
    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))
    (1): AdaptiveAvgPool2d(output_size=1)
    (2): Conv2d(128, 441, kernel_size=(1, 1), stride=(1, 1))
    (3): Softmax(dim=None)
  )
)
24-10-07 11:50:37.162 : Network D structure: DataParallel - UNetDiscriminatorSN, with parameters: 4,376,897
24-10-07 11:50:37.162 : UNetDiscriminatorSN(
  (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
24-10-07 11:50:37.162 : Network F structure: PerceptualLoss, with parameters: 20,024,384
24-10-07 11:50:37.162 : PerceptualLoss(
  (vgg): VGGFeatureExtractor(
    (vgg_net): Sequential(
      (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu1_1): ReLU(inplace=True)
      (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu1_2): ReLU(inplace=True)
      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu2_1): ReLU(inplace=True)
      (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu2_2): ReLU(inplace=True)
      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3_1): ReLU(inplace=True)
      (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3_2): ReLU(inplace=True)
      (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3_3): ReLU(inplace=True)
      (conv3_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3_4): ReLU(inplace=True)
      (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu4_1): ReLU(inplace=True)
      (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu4_2): ReLU(inplace=True)
      (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu4_3): ReLU(inplace=True)
      (conv4_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu4_4): ReLU(inplace=True)
      (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu5_1): ReLU(inplace=True)
      (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu5_2): ReLU(inplace=True)
      (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu5_3): ReLU(inplace=True)
      (conv5_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (criterion): L1Loss()
)
24-10-07 11:50:37.162 : ===================== start training =====================
24-10-07 11:50:37.163 : ===================== resume training =====================
24-10-07 11:50:37.163 : ===> No saved training states to resume.
24-10-07 11:50:37.163 : ===> start training from epoch: 0, iter: 0.
24-10-07 11:50:37.163 : Total # of epochs for training: 500000.
24-10-07 11:50:37.163 : ===> train:: Epoch[1]
24-10-07 11:50:37.309 : ===================== start testing =====================
24-10-07 11:52:26.908 : ===================== Selected training parameters =====================
24-10-07 11:52:26.908 : Namespace(opt='/home/asif/Documents/kernel_est/experiment5/pl_idenet/setting1/train/train_setting1_x4.yml', color=True, wiener_kernel_size=(5, 5), wiener_output_features=24, numWienerFilters=4, wienerWeightSharing=True, wienerChannelSharing=True, alphaChannelSharing=True, alpha_update=True, lb=1e-05, ub=0.01, wiener_pad=True, wiener_padType='symmetric', edgeTaper=True, wiener_scale=False, wiener_normalizedWeights=True, wiener_zeroMeanWeights=True, kernel_size=(5, 5), conv_init='dct', pad='same', padType='symmetric', in_nc=3, nf=64, resdnet_depth=5, patch=1, embed_dim=96, depths=[4, 4, 4, 4], num_heads=[4, 4, 4, 4], window_size=8, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.1, ape=False, patch_norm=True, use_checkpoint=False, upscale=4, img_range=1.0, upsampler='', resi_connection='1conv', train_stdn=[0.0], test_stdn=[0.0], upscale_factor=4, trainBatchSize=2, testBatchSize=1, niter=500000, use_bn=False, cuda=True, gpu_ids=[0], seed=123, use_filters=False, resume=True, resume_start_epoch=0, pretrainedModelPath='pretrained_nets/idenet/2853_final_G.pth', pretrain=False, tile=128, tile_overlap=32, whole=True, imdbTrainPath='datasets/', imdbTestPath='datasets/', patch_size=32, rgb_range=1, is_train=True, is_mixup=True, use_chop=False, alpha=1.2, numWorkers=4, lr_G=0.0001, beta1_G=0.9, beta2_G=0.999, eps_G=1e-08, weightdecay_G=0, lr_D=0.0001, beta1_D=0.9, beta2_D=0.999, eps_D=1e-08, weightdecay_D=0, amsgrad=False, lr_milestones=[250000, 400000, 450000, 475000], lr_gamma=0.5, lr_restart=None, lr_restart_weights=None, warmup_iter=-1, D_update_ratio=1, D_init_iters=0, pixel_criterion='l1', pixel_criterion_char='charbonnair', pixel_criterion_kernel='l1', pixel_criterion_GT_kernel='l1', feature_criterion='l1', tv_criterion='l1', gan_type='gan', pixel_weight=1.0, pixel_kernel_weight=1.0, pixel_weight_char=1.0, pixel_kernel_gt_weight=1.0, feature_weight=1.0, layer_weights={'conv1_2': 0.1, 'conv2_2': 0.1, 'conv3_4': 1, 'conv4_4': 1, 'conv5_4': 1}, tv_weight=1.0, gan_weight=0.1, saveTrainedModelsPath='trained_nets', save_path_training_states='/training_states/', save_path_netG='/netG/', save_path_netD='/netD/', save_path_best_psnr='/best_psnr/', save_path_best_lpips='/best_lpips/', saveImgsPath='results', saveLogsPath='logs', save_checkpoint_freq=50, saveBest=True).
24-10-07 11:52:26.908 : ===================== Loading dataset =====================
24-10-07 11:52:26.909 : training dataset:     1
24-10-07 11:52:26.909 : training loaders:     0
24-10-07 11:52:26.909 : testing dataset:     1
24-10-07 11:52:26.909 : testing loaders:     1
24-10-07 11:52:26.909 : ===================== Building model =====================
24-10-07 11:52:29.206 : Network G structure: DataParallel - DEblurSRResDNet, with parameters: 4,307,982
24-10-07 11:52:29.206 : DEblurSRResDNet(
  (netDe): SRResDNet(
    (netT): SwinIR(
      (conv_first): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(
        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      )
      (patch_unembed): PatchUnEmbed()
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): RSTB(
          (residual_group): BasicLayer(
            dim=96, input_resolution=(32, 32), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.007)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.013)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.020)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(224, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
          (pgm): PromptGenBlock(
            (linear_layer): Linear(in_features=96, out_features=128, bias=True)
            (conv3x3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (pim): PromptInteractionBlock(
            (transformerblock): TransformerBlock(
              (norm1): LayerNorm(
                (body): WithBias_LayerNorm()
              )
              (attn): Attention(
                (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
                (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              )
              (norm2): LayerNorm(
                (body): WithBias_LayerNorm()
              )
              (ffn): FeedForward(
                (project_in): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
                (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              )
            )
          )
        )
        (1): RSTB(
          (residual_group): BasicLayer(
            dim=96, input_resolution=(32, 32), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.027)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.033)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.040)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.047)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
          (pgm): PromptGenBlock(
            (linear_layer): Linear(in_features=96, out_features=128, bias=True)
            (conv3x3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (pim): PromptInteractionBlock(
            (transformerblock): TransformerBlock(
              (norm1): LayerNorm(
                (body): WithBias_LayerNorm()
              )
              (attn): Attention(
                (qkv): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (qkv_dwconv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
                (project_out): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              )
              (norm2): LayerNorm(
                (body): WithBias_LayerNorm()
              )
              (ffn): FeedForward(
                (project_in): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (dwconv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
                (project_out): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              )
            )
          )
        )
        (2): RSTB(
          (residual_group): BasicLayer(
            dim=96, input_resolution=(32, 32), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.053)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.060)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.067)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.073)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
          (pgm): PromptGenBlock(
            (linear_layer): Linear(in_features=96, out_features=128, bias=True)
            (conv3x3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (pim): PromptInteractionBlock(
            (transformerblock): TransformerBlock(
              (norm1): LayerNorm(
                (body): WithBias_LayerNorm()
              )
              (attn): Attention(
                (qkv): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (qkv_dwconv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
                (project_out): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              )
              (norm2): LayerNorm(
                (body): WithBias_LayerNorm()
              )
              (ffn): FeedForward(
                (project_in): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (dwconv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
                (project_out): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              )
            )
          )
        )
        (3): RSTB(
          (residual_group): BasicLayer(
            dim=96, input_resolution=(32, 32), depth=4
            (blocks): ModuleList(
              (0): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.080)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (1): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.087)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (2): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.093)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
              (3): SwinTransformerBlock(
                dim=96, input_resolution=(32, 32), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0
                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attn): WindowAttention(
                  dim=96, window_size=(8, 8), num_heads=4
                  (qkv): Linear(in_features=96, out_features=288, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=96, out_features=96, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (softmax): Softmax(dim=-1)
                )
                (drop_path): DropPath(drop_prob=0.100)
                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=96, out_features=384, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=384, out_features=96, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (conv): Conv2d(256, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (patch_embed): PatchEmbed()
          (patch_unembed): PatchUnEmbed()
          (pgm): PromptGenBlock(
            (linear_layer): Linear(in_features=96, out_features=128, bias=True)
            (conv3x3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (pim): PromptInteractionBlock(
            (transformerblock): TransformerBlock(
              (norm1): LayerNorm(
                (body): WithBias_LayerNorm()
              )
              (attn): Attention(
                (qkv): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (qkv_dwconv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
                (project_out): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              )
              (norm2): LayerNorm(
                (body): WithBias_LayerNorm()
              )
              (ffn): FeedForward(
                (project_in): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (dwconv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
                (project_out): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              )
            )
          )
        )
      )
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (conv_after_body): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_last): Conv2d(96, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (deblur): WienerDeblurNet(, wiener_kernel_size = (5, 5), wiener_output_features = 24, wienerWeightSharing = True, edgeTaper = True
    (noise_estimator): Wmad_estimator()
    (k_embd): M_kemb(
      (linear): Linear(in_features=441, out_features=128, bias=True)
      (act): ReLU()
    )
    (conv_before_upsample): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (upsample): Upsample(
      (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): PixelShuffle(upscale_factor=2)
      (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): PixelShuffle(upscale_factor=2)
    )
    (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (netK): KernelEstimator(
    (conv_1): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
    )
    (res_block_1): Sequential(
      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (max_pool_1): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (conv_2): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
    )
    (res_block_2): Sequential(
      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (maxpool_2): Sequential(
      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (conv_3): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
    )
    (transpose_1): Sequential(
      (0): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
    )
    (conv_4): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
    )
    (res_block_3): Sequential(
      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (transpose_2): Sequential(
      (0): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
    )
    (conv_5): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
    )
    (res_block_4): Sequential(
      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (conv_6): Sequential(
      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))
      (1): AdaptiveAvgPool2d(output_size=1)
      (2): Conv2d(128, 441, kernel_size=(1, 1), stride=(1, 1))
      (3): Softmax(dim=None)
    )
  )
)
24-10-07 11:52:29.207 : Network K structure: KernelEstimator, with parameters: 350,137
24-10-07 11:52:29.207 : KernelEstimator(
  (conv_1): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
  )
  (res_block_1): Sequential(
    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (max_pool_1): Sequential(
    (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv_2): Sequential(
    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
  )
  (res_block_2): Sequential(
    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (maxpool_2): Sequential(
    (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (conv_3): Sequential(
    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
  )
  (transpose_1): Sequential(
    (0): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
  )
  (conv_4): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
  )
  (res_block_3): Sequential(
    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (transpose_2): Sequential(
    (0): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
  )
  (conv_5): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
  )
  (res_block_4): Sequential(
    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (conv_6): Sequential(
    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))
    (1): AdaptiveAvgPool2d(output_size=1)
    (2): Conv2d(128, 441, kernel_size=(1, 1), stride=(1, 1))
    (3): Softmax(dim=None)
  )
)
24-10-07 11:52:29.207 : Network D structure: DataParallel - UNetDiscriminatorSN, with parameters: 4,376,897
24-10-07 11:52:29.207 : UNetDiscriminatorSN(
  (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
24-10-07 11:52:29.208 : Network F structure: PerceptualLoss, with parameters: 20,024,384
24-10-07 11:52:29.208 : PerceptualLoss(
  (vgg): VGGFeatureExtractor(
    (vgg_net): Sequential(
      (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu1_1): ReLU(inplace=True)
      (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu1_2): ReLU(inplace=True)
      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu2_1): ReLU(inplace=True)
      (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu2_2): ReLU(inplace=True)
      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3_1): ReLU(inplace=True)
      (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3_2): ReLU(inplace=True)
      (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3_3): ReLU(inplace=True)
      (conv3_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu3_4): ReLU(inplace=True)
      (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu4_1): ReLU(inplace=True)
      (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu4_2): ReLU(inplace=True)
      (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu4_3): ReLU(inplace=True)
      (conv4_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu4_4): ReLU(inplace=True)
      (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu5_1): ReLU(inplace=True)
      (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu5_2): ReLU(inplace=True)
      (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu5_3): ReLU(inplace=True)
      (conv5_4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (criterion): L1Loss()
)
24-10-07 11:52:29.208 : ===================== start training =====================
24-10-07 11:52:29.208 : ===================== resume training =====================
24-10-07 11:52:29.208 : ===> No saved training states to resume.
24-10-07 11:52:29.208 : ===> start training from epoch: 0, iter: 0.
24-10-07 11:52:29.208 : Total # of epochs for training: 500000.
24-10-07 11:52:29.209 : ===> train:: Epoch[1]
24-10-07 11:52:29.347 : ===================== start testing =====================
